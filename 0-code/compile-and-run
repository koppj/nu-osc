#!/bin/bash

# -----------------------------------------------------------------
function on_exit()
# -----------------------------------------------------------------
# Graceful exit
# -----------------------------------------------------------------
{
  if [[ -e mb-orig.cc ]]; then
    echo "**********************************************************************"
    echo "WARNING: #define's in mb.cc may be in undefined state"
    echo "**********************************************************************"
    mv mb-orig.cc mb.cc
    touch mb.cc
  fi
  if [[ -e reactors/definitions-orig.h ]]; then
    echo "**********************************************************************"
    echo "WARNING: #define's in reactors/definitions.h may be in undefined state"
    echo "**********************************************************************"
    mv reactors/definitions-orig.h reactors/definitions.h
    touch reactors/definitions.h
  fi
  exit 0
}


# -----------------------------------------------------------------
#                     M A I N   S C R I P T
# -----------------------------------------------------------------

trap on_exit HUP QUIT KILL ABRT SEGV INT TERM EXIT

# Parse command line options
RUN_LOCALLY=false
RUN_MPI=false
while getopts "lM" opt; do
  case $opt in
    l)
       RUN_LOCALLY=true
      ;;
    M)
       RUN_MPI=true
     ;;
  esac
done
shift $((OPTIND-2))


# Pick name for temporary executable
EXE=$(mktemp -u nu-tmp-XXXXX)
SLURM_SCRIPT=nu-script${EXE#nu-tmp}

# Definitions for Thomas' SBL codes
if [[ -z $SBLDEFS ]]; then
  SBLDEFS="SBL CHOOZ PV DC DB KAML RENO BUGEY_SP GAL DANSS DB_FLUX DB_ALVARO NEOS_ALVARO"
fi
CODE=""
for D in $SBLDEFS; do
  CODE="$CODE\n#define USE_"$D
done
echo "# Extra define's for SBL codes: " $SBLDEFS
cp reactors/definitions.h reactors/definitions-orig.h
sed -i "/#define USE_.*/d;s/\/\/ TAG_DEFS.*/$CODE/" reactors/definitions.h

# Definitions for Pedro's MiniBooNE code
if [[ -z $MBDEFS ]]; then
  MBDEFS="NU_APP NU_DISAPP NUBAR_APP NUBAR_DISAPP COMBINED_APP FULL_OSC"
fi
CODE=""
for D in $MBDEFS; do
  CODE="$CODE\n#define "$D
done
echo "# Extra define's for Pedro's MiniBooNE codes: " $MBDEFS
cp mb.cc mb-orig.cc
sed -i "/#define.*\/\/ TAG_DEF/d;s/\/\/ TAG_DEFS.*/$CODE/" mb.cc

# Definitions for Joachim's MiniBooNE code
if [[ -z $MBJKDEFS ]]; then
  MBJKDEFS="OSC_NORM OSC_BG OSC_NUMU"
fi
CODE=""
for D in $MBJKDEFS; do
  CODE="$CODE\n#define "$D
done
echo "# Extra define's for Joachim's MiniBooNE codes: " $MBJKDEFS
cp mb-jk.cc mb-jk-orig.cc
sed -i "/#define.*\/\/ TAG_DEF/d;s/\/\/ TAG_DEFS.*/$CODE/" mb-jk.cc


# Compilation
echo "# Compiling ..."
#make clean > /dev/null
make -j 2>&1 > /dev/null
mv nu $EXE

mv reactors/definitions-orig.h reactors/definitions.h
touch reactors/definitions.h
mv mb-orig.cc mb.cc
touch mb.cc
mv mb-jk-orig.cc mb-jk.cc
touch mb-jk.cc


# Run - possibly in parallel
if [[ -z $NU_SIZE ]]; then
  export NU_SIZE=1;
fi

OF=$OUT_FILE

# MPI parallelization
if $RUN_MPI; then
  if $RUN_LOCALLY; then
    echo "# Running job $EXE locally in MPI mode, $NU_SIZE processes ..."
    unbuffer mpirun -np $NU_SIZE ./$EXE $@ > ../results/$OUT_FILE &
  else
    echo "# Submitting job $EXE in MPI mode, output file $OUT_FILE ..."
    NU_COMMAND="mpirun -np $NU_SIZE ./$EXE $@ > ../results/$OUT_FILE"
  fi

# Manual parallelization
elif [[ -z $NU_RANK ]]; then
  for (( I=0; $I < $NU_SIZE; I++ )); do
    export NU_RANK=$I
    export NU_SIZE
    if (( $NU_SIZE > 1 )); then
      export OUT_FILE="${OF%%.dat}_${NU_RANK}.dat"
    fi

    if [[ -f ../results/$OUT_FILE ]]; then
      echo "output file $OUTPUT_FILE exists. skipping slice $NU_RANK."
      continue
    fi

    if $RUN_LOCALLY; then
      echo "# Running job $EXE locally ..."
      unbuffer ./$EXE $@ > ../results/$OUT_FILE &
    else
      echo "# Submitting job $EXE, output file $OUT_FILE ..."
      NU_COMMAND="./$EXE $@ > ../results/$OUT_FILE"

      # For SLURM:
      #   -A <account>
      #   -p <partition/queue>
      #   -n <# of tasks/cores>
      #   -N <# of nodes>
      #   -J <jobname>
      cat > $SLURM_SCRIPT-${NU_RANK} << EOF
#!/bin/bash
# SLURM script for sterile neutrino code
# Mona Dentler, Joachim Kopp (JGU Mainz), 2017
#
#SBATCH -J ${SLURM_SCRIPT#nu-script} # Job name
#SBATCH -o ../results/$OUT_FILE  # Specify stdout output file (%j expands to jobId)
#SBATCH -p long                  # Queue name
#SBATCH -N 1                     # Total number of nodes requested (16 cores/node)
#SBATCH -n 1                     # Total number of tasks
#SBATCH -A thep                  # Specify allocation to charge against
#SBATCH -t 2-0                   # run time up to one day

# Load all necessary modules if needed (these are examples)
# Loading modules in the script ensures a consistent environment.
module load compiler/GCC/6.3.0
module load numlib/GSL/2.5-iccifort-2018.3.222-GCC-6.3.0
module load data/HDF5/1.10.2-intel-2018.03
 
# Launch the executable
NU_SIZE=$NU_SIZE NU_RANK=$NU_RANK $NU_COMMAND
EOF
      sbatch $SLURM_SCRIPT-${NU_RANK}
    fi
  done
else
  export NU_RANK
  export NU_SIZE
  if (( $NU_SIZE > 1 )); then
    export OUT_FILE="${OF%%.dat}_${NU_RANK}.dat"
  fi

  # Submit to MOGON
  #   -q <queue>
  #   -n <no of cores>
  #   -o <name of output file (stdout, stderr)>
  #   -N            --   Send email when job is finished
  #   -W <hrs:min>  --   Maximum wall clock time (hrs:min)
  #   -M xxx        --   Memory limit
  if $RUN_LOCALLY; then
    echo "# Running job $EXE locally ..."
    ./$EXE $@ | tee ../results/$OUT_FILE &
  else
    echo "# Submitting job $EXE, output file $OUT_FILE ..."
    NU_COMMAND="./$EXE $@ > ../results/$OUT_FILE"
  fi
fi


# If running on cluster, create run script if necessary and submit to queue
if [ $RUN_LOCALLY = false ]; then
  # For LFS:
  #   -q <queue>
  #   -n <no of cores>
  #   -o <name of output file (stdout, stderr)>
  #   -N            --   Send email when job is finished
  #   -W <hrs:min>  --   Maximum wall clock time (hrs:min)
  #   -M xxx        --   Memory limit
#  bsub -q theplong \
#       -n 1 \
#       -N \
#       -W 96:00 \
#       -M 1000000 \
#       $NU_COMMAND

  # For SLURM:
  #   -A <account>
  #   -p <partition/queue>
  #   -n <# of tasks/cores>
  #   -N <# of nodes>
  #   -J <jobname>
  cat > $SLURM_SCRIPT << EOF
#!/bin/bash
# SLURM script for sterile neutrino code
# Mona Dentler, Joachim Kopp (JGU Mainz), 2017
#
#SBATCH -J ${SLURM_SCRIPT#nu-script} # Job name
#SBATCH -o ../results/$OUT_FILE  # Specify stdout output file (%j expands to jobId)
#SBATCH -p long                  # Queue name
#SBATCH -N 1                     # Total number of nodes requested (16 cores/node)
#SBATCH -n $NU_SIZE              # Total number of tasks
#SBATCH -A thep                  # Specify allocation to charge against
#SBATCH -t 2-0                   # run time up to one day

# Load all necessary modules if needed (these are examples)
# Loading modules in the script ensures a consistent environment.
module load compiler/GCC/6.3.0
module load numlib/GSL/2.5-iccifort-2018.3.222-GCC-6.3.0
module load data/HDF5/1.10.2-intel-2018.03
 
# Launch the executable
$NU_COMMAND
EOF

  sbatch $SLURM_SCRIPT
fi

##  #SBATCH --mem-per-cpu=500        # Memory requested per CPU [MB]

